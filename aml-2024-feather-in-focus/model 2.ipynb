{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9652abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09ecf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-s (Function  (None, 13, 13, 1280)      20331360  \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_3  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               256200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20587560 (78.54 MB)\n",
      "Trainable params: 256200 (1000.78 KB)\n",
      "Non-trainable params: 20331360 (77.56 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the saved EfficientNetV2 model\n",
    "model = load_model(\"efficientnetv2_model.h5\")\n",
    "\n",
    "# Check the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d176ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model's layers (optional, depending on your needs)\n",
    "for layer in model.layers[:-1]:  # Exclude the output layer\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # Fine-tuning requires a smaller learning rate\n",
    "    loss=\"categorical_crossentropy\",     # Ensure the loss matches your use case\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f0734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 312)\n"
     ]
    }
   ],
   "source": [
    "# load class_names\n",
    "labels = np.load(\"class_names.npy\", allow_pickle=True).item()\n",
    "\n",
    "# load attributes\n",
    "attributes = np.load(\"attributes.npy\")\n",
    "print(attributes.shape)\n",
    "attributes_dict = {i + 1: row for i, row in enumerate(attributes)}\n",
    "# load training data\n",
    "train_df = pd.read_csv(\"train_images.csv\")\n",
    "# add attributes to data\n",
    "train_df[\"attributes\"] = train_df[\"label\"].map(attributes_dict)\n",
    "\n",
    "# Initialize empty lists for images (X) and labels (y)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate over rows in the DataFrame\n",
    "for _, row in train_df.iterrows():\n",
    "    # Read the image\n",
    "    im_path = row['image_path']\n",
    "    image = cv2.imread(f'train_images{im_path}')\n",
    "    \n",
    "    # Resize the image to 400x400\n",
    "    image_resized = cv2.resize(image, (400, 400))\n",
    "    \n",
    "    # Append to the list of images and labels\n",
    "    X.append(image_resized)\n",
    "\n",
    "# Convert lists to numpy arrays (optional)\n",
    "X = np.array(X)\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32fc842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3140, 200)\n"
     ]
    }
   ],
   "source": [
    "# Convert y to categorical\n",
    "y_c = to_categorical(y.values-1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_c, test_size=0.2, random_state=0)\n",
    "# use one-hot encoding for labels\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a95537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.5608571428571428, 1: 0.5608571428571428, 2: 0.5948484848484848, 3: 0.5608571428571428, 4: 1.033157894736842, 5: 1.226875, 6: 0.7010714285714286, 7: 0.8534782608695652, 8: 0.5773529411764706, 9: 0.5773529411764706, 10: 0.5948484848484848, 11: 0.6332258064516129, 12: 0.5948484848484848, 13: 0.5948484848484848, 14: 0.5948484848484848, 15: 0.5948484848484848, 16: 0.6134375, 17: 0.9815, 18: 0.6134375, 19: 0.6134375, 20: 0.6134375, 21: 0.6332258064516129, 22: 0.6134375, 23: 0.727037037037037, 24: 0.6332258064516129, 25: 0.6332258064516129, 26: 0.6332258064516129, 27: 0.6332258064516129, 28: 0.6332258064516129, 29: 0.6332258064516129, 30: 0.6543333333333333, 31: 0.7010714285714286, 32: 0.6543333333333333, 33: 0.6543333333333333, 34: 0.6543333333333333, 35: 0.6543333333333333, 36: 0.6543333333333333, 37: 0.676896551724138, 38: 0.676896551724138, 39: 0.676896551724138, 40: 0.676896551724138, 41: 0.676896551724138, 42: 0.676896551724138, 43: 0.676896551724138, 44: 0.7010714285714286, 45: 0.7010714285714286, 46: 0.7010714285714286, 47: 0.7010714285714286, 48: 0.7010714285714286, 49: 0.7010714285714286, 50: 0.727037037037037, 51: 0.727037037037037, 52: 0.727037037037037, 53: 0.727037037037037, 54: 0.727037037037037, 55: 0.727037037037037, 56: 0.727037037037037, 57: 0.755, 58: 0.755, 59: 0.755, 60: 0.755, 61: 0.755, 62: 0.755, 63: 0.755, 64: 0.7852, 65: 0.7852, 66: 0.7852, 67: 0.7852, 68: 0.7852, 69: 0.7852, 70: 0.8179166666666666, 71: 0.8179166666666666, 72: 0.8179166666666666, 73: 0.8179166666666666, 74: 0.8179166666666666, 75: 0.8179166666666666, 76: 0.8179166666666666, 77: 0.8534782608695652, 78: 0.8534782608695652, 79: 0.8534782608695652, 80: 0.8534782608695652, 81: 0.8534782608695652, 82: 0.8534782608695652, 83: 0.8922727272727272, 84: 0.8922727272727272, 85: 0.8922727272727272, 86: 0.8922727272727272, 87: 0.8922727272727272, 88: 0.8922727272727272, 89: 0.8922727272727272, 90: 0.9347619047619048, 91: 0.9347619047619048, 92: 0.9347619047619048, 93: 0.9347619047619048, 94: 0.9347619047619048, 95: 0.9347619047619048, 96: 0.9347619047619048, 97: 0.9815, 98: 0.9815, 99: 0.9815, 100: 0.9815, 101: 0.9815, 102: 0.9815, 103: 1.033157894736842, 104: 1.033157894736842, 105: 1.033157894736842, 106: 1.033157894736842, 107: 1.033157894736842, 108: 1.033157894736842, 109: 1.033157894736842, 110: 1.0905555555555555, 111: 1.0905555555555555, 112: 1.0905555555555555, 113: 1.0905555555555555, 114: 1.0905555555555555, 115: 1.0905555555555555, 116: 1.0905555555555555, 117: 1.1547058823529412, 118: 1.1547058823529412, 119: 1.1547058823529412, 120: 1.1547058823529412, 121: 1.1547058823529412, 122: 1.1547058823529412, 123: 1.226875, 124: 1.226875, 125: 1.226875, 126: 1.226875, 127: 1.226875, 128: 1.226875, 129: 1.226875, 130: 1.3086666666666666, 131: 1.3086666666666666, 132: 1.3086666666666666, 133: 1.3086666666666666, 134: 1.3086666666666666, 135: 1.3086666666666666, 136: 1.3086666666666666, 137: 1.4021428571428571, 138: 1.4021428571428571, 139: 1.4021428571428571, 140: 1.4021428571428571, 141: 1.4021428571428571, 142: 1.4021428571428571, 143: 1.51, 144: 1.51, 145: 1.51, 146: 1.51, 147: 1.51, 148: 1.51, 149: 1.51, 150: 1.6358333333333333, 151: 1.6358333333333333, 152: 1.6358333333333333, 153: 1.6358333333333333, 154: 1.6358333333333333, 155: 1.6358333333333333, 156: 1.7845454545454544, 157: 1.7845454545454544, 158: 1.7845454545454544, 159: 1.7845454545454544, 160: 1.7845454545454544, 161: 1.7845454545454544, 162: 1.7845454545454544, 163: 1.963, 164: 1.963, 165: 1.963, 166: 1.963, 167: 1.963, 168: 1.963, 169: 1.963, 170: 2.181111111111111, 171: 2.181111111111111, 172: 2.181111111111111, 173: 2.181111111111111, 174: 2.181111111111111, 175: 2.181111111111111, 176: 2.45375, 177: 2.45375, 178: 2.45375, 179: 2.45375, 180: 2.45375, 181: 2.45375, 182: 2.45375, 183: 2.8042857142857143, 184: 2.8042857142857143, 185: 2.8042857142857143, 186: 2.8042857142857143, 187: 2.8042857142857143, 188: 2.8042857142857143, 189: 2.8042857142857143, 190: 3.2716666666666665, 191: 3.2716666666666665, 192: 3.2716666666666665, 193: 3.2716666666666665, 194: 3.2716666666666665, 195: 3.2716666666666665, 196: 3.926, 197: 3.926, 198: 3.926, 199: 3.926}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight = 'balanced', \n",
    "    classes = np.unique(y), \n",
    "    y = y\n",
    ")\n",
    "\n",
    "# Convert to a dictionary for Keras\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2bcebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "99/99 [==============================] - 776s 8s/step - loss: 0.8150 - accuracy: 0.8952 - val_loss: 1.6316 - val_accuracy: 0.5954\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 503s 5s/step - loss: 0.7981 - accuracy: 0.8949 - val_loss: 1.6290 - val_accuracy: 0.5891\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 512s 5s/step - loss: 0.7784 - accuracy: 0.8971 - val_loss: 1.6291 - val_accuracy: 0.5878\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 518s 5s/step - loss: 0.7686 - accuracy: 0.8968 - val_loss: 1.6274 - val_accuracy: 0.5878\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 506s 5s/step - loss: 0.7527 - accuracy: 0.9096 - val_loss: 1.6232 - val_accuracy: 0.5840\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6619636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 97s 4s/step - loss: 1.6301 - accuracy: 0.5980\n",
      "Test Loss: 1.6301114559173584, Test Accuracy: 0.5979644060134888\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21473c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 646s 5s/step\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test_images_path.csv\")\n",
    "\n",
    "test_images = []\n",
    "# Iterate over rows in the DataFrame\n",
    "for _, row in test_df.iterrows():\n",
    "    # Read the image\n",
    "    im_path = row['image_path']\n",
    "    image = cv2.imread(f'test_images{im_path}')\n",
    "    \n",
    "    # Resize the image to 400x400\n",
    "    image_resized = cv2.resize(image, (400, 400))\n",
    "    \n",
    "    # Append to the list of images and labels\n",
    "    test_images.append(image_resized)\n",
    "\n",
    "\n",
    "predictions = model.predict(np.array(test_images))\n",
    "predicted_labels = np.argmax(predictions, axis = 1)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'label': predicted_labels + 1\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcbf3a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(min(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3f890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
